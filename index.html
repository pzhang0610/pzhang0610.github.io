 <!DOCTYPE html>

<html><head>
<title>Peng Zhang - Shandong University of Science and Technology</title>

<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>

<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");


	body
	{
	font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:14px;line-height:1.5;font-weight:300;
    	background-color : #CDCDCD;
	}
    	.content
	{
    		width : 900px;
    		padding : 25px 30px;
    		margin : 25px auto;
    		background-color : #fff;
    		box-shadow: 0px 0px 10px #999;
    		border-radius: 15px; 
	}	
	table
	{
		padding: 5px;
	}
	
	table.pub_table,td.pub_td1,td.pub_td2
	{
		padding: 8px;
		width: 850px;
        border-collapse: separate;
        border-spacing: 15px;
        margin-top: -5px;
	}

	td.pub_td1
	{
		width:50px;
	}
    td.pub_td1 img
    {
        height:120px;
        width: 160px;
    }
	
	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 820px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #1367a7;
		height: 158px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h4
	{
		color: #000000;
		font-size:100%;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
    }
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}

	#sdst_logo {
		position: absolute;
		left: 560px;
		top: 14px;
		width: 20px;
		height: 20px;
	}
	#uts_logo {
        position: absolute;
        left: 646px;
        top: 14px;
        width: 100px;
        height: 20px;
    }
   
    table.pub_table tr {
        outline: thin dotted #666666;
    }
    .papericon {
        border-radius: 8px; 
        -moz-box-shadow: 3px 3px 6px #888;
        -webkit-box-shadow: 3px 3px 6px #888;
        box-shadow: 3px 3px 6px #888;
	height:120px;
        width: 160px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }
    .media {
	outline: thin dotted #666666;
 	margin-bottom: 15px;	
	margin-left:10px;
    }
    .media-body {
	margin-top:5px;
	padding-left:20px;
    }

.papers-selected h5, .papers-selected h4 { display : none; }
.papers-selected .publication { display : none; }
.paperhi-only { display : none; }
.papers-selected .paperhi { display : flex; }
.papers-selected .paperlo { display : none; }

.hidden>div {
	display:none;
}

.visible>div {
	display:block;
}
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-23931362-2');
</script>

<script type="text/javascript">
    var myPix = new Array("imgs/profile.jpeg","imgs/profile.jpeg")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };
</script>

<script>
$(document).ready(function() {
  $('.paperlo button').click(function() {
     $('.papers-container').addClass('papers-selected');
  });
  $('.paperhi button').click(function() {
     $('.papers-container').removeClass('papers-selected');
  });
  $('.papers-container').removeClass('papers-selected');

		$('.text_container').addClass("hidden");

		$('.text_container').click(function() {
			var $this = $(this);

			if ($this.hasClass("hidden")) {
				$(this).removeClass("hidden").addClass("visible");

			} else {
				$(this).removeClass("visible").addClass("hidden");
			}
		});

});
</script>

</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="imgs/profile.jpeg" style="float:left; padding-right:20px" width="200px" height="200px"></td>
	<script>choosePic();</script>
	<td>
	<div id="DocInfo">
		<h1>Peng Zhang</h1>
		Ph.D, Associate Professor<br>
		Department of Artificial Intelligence,<br>
		Shandong University of Science and Technology <br>
        Office: J13-322, 579 Qianwangang Road, Qingdao, 266590 <br>
		Email: <a href="pengzhang_skd@sdust.edu.cn">pengzhang_skd [at] sdust.edu.cn</a><br>
        <a href="CV_web.pdf">CV</a> &bull; <a href="https://scholar.google.com/citations?user=inYJJjoAAAAJ&hl=en">Google Scholar</a> &bull; <a href="https://github.com/pzhang0610">Github</a>&bull; <a href="https://cise.sdust.edu.cn/home/Page/dep_detail/catId/73/id/1787.html">主页</a> <br>
	</div><br>
  
     <div id="sdst_logo">
        <img src="imgs/sdst.jpg"  width="80" height="80"/></a>
	</div>

    <div id="uts_logo">
        <img src="imgs/uts_logo.png" width="160" height="80"/>
	</div>

	</td>
	</tr>
	</tbody></table>
	<br>

	<h2>About Me</h2>
	<ul>
	Greetings! I am an associate professor in <a href="https://en.sdust.edu.cn">Shandong University of Science and Technology (SDUST)</a>. I received my PhD degree from <a href="https://www.uts.edu.au">University of Technology Sydney (UTS)</a> supervised by A/Prof. <a href="https://www.uts.edu.au/staff/qiang.wu">Qiang Wu</a> and Dr. <a href="https://www.uts.edu.au/staff/jingsong.xu">Jingsong Xu</a> in Aug. 2020.
			Before that, I received B.E. and M.E. from <a href="http://www.en.sdu.edu.cn">Shandong University</a> supervised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=dyGlWmkAAAAJ">Xianye Ben</a> in 2013 and 2016, respectively. Besides, 
			I am a visiting student at <a href="http://en.xjtu.edu.cn/index.htm">Xi'an Jiaotong University</a> during Aug. 2010 - Jul. 2011. My research interest includes human-centric visual understanding, such as gait recognition  and
			person re-identification, and afftective computing, such as micro-expression recognition and multi-modal emotion perceptron, etc.
	</ul>

	<!--
	<h2>Research</h2>
    <ul>
        <li>Computer Vision, Machine Learning, Gait recognition, Person re-identification, GAN</li> 

    </ul>
	-->
    <h2>News</h2>
    <ul>
	        <li> [2025/03/14] 1 co-authored paper is accepted to Pattern Recognition. </li>
	        <li> [2025/01/24] I'm entitled as a Young Editor for Journal of Industrial Safety (<a href="https://xuebao.sdust.edu.cn/">山东科技大学学报（自然科学版）</a>)</li>
	        <li> [2024/09/25] 1 paper is accepted to IEEE TCSVT. Congrats. to Xinyu! </li>
	        <li> [2024/09/19] I'm entitled CCF Senior Member. </li>
	        <li> [2024/09/17] 1 paper is accepted to Chinese Conference on Biometric Recognition (CCBR 2024). Congrats to Hao!</li>
	        <li> [2024/09/02] CFP: We launched a special issue <a href="https://mp.weixin.qq.com/s/sejj7DGn7O3FXaiQ0c1zlQ"> Multimodal Image-based Object Detection and Recognition </a> on Journal of Beijing Institute of Technology (北京理工大学学报英文版).</li>
		<li> [2024/05/10] 1 paper about micro-expression recognition is accepted to IEEE Trans. Affective Computing. </li>
	        <li> [2024/03/16] I paper is accepted to IJCNN 2024. </li>
	        <li> [2022/11/23] I am invited as a reviewer for 2023 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP 2023). </li>
		<li> [2022/10/30] 1 co-authered paper is accepted to IEEE Trans. KDE, congrats to Dr. Li. </li>
		<li> [2022/10/02] 1 co-authored paper is accepted to Pattern Recognition Letters(PRL). </li>
		<li> [2022/08/16] I am invited as a TPC member for AAAI-2023.</li> 
    </ul>


 <h2>Academic Services</h2>
	<div>
    <ul>
        <li>Conference reviewer for
			<ul>
				<li> ACM Multimedia Asia 2024</li>
				<li>International Joint Conference on Neural Networks (IJCNN 2024)</li>
				<li> IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP 2023) </li>
				<li>International Conference on Digital Image Computing: Techniques and Applications (DICTA 2024, 2023, 2022)</li>
				<li>AAAI Conference on Artificial Intelligence (AAAI 2023) </li>
				<li>35th Conference on Neural Information Processing Systems (NeurIPS 2021)</li>
				<li>International Conference on Multimedia and Expo (ICME), 2022, 2021, 2020</li>
				<li>International Conference on Learning Representations (ICLR),2023, 2022, 2021</li>
				<li>International Conference on Machine Learning (ICML), 2022, 2020</li> 
				<li>International Conference on Visual Communication and Image Processing (VCIP),2022, 2020, 2019</li>
				<li>International Conference on Machine Learning and Data Science (ICMLDS), 2019,2018</li>
			</ul>
		</li>
	    <li>Journal reviewer for 
			<ul>
				<li>IEEE Transactions on Image Processing</li>
				<li>IEEE Transactions on Multimedia </li>
				<li>IEEE Transactions on Circuits and Systems for Video Technology </li>
				<li>IEEE Signal Processing Letters</li> 
				<li>IEEE Journal of Biomedical and Health Informatics</li> 
				<li>Neurocomputing</li>
				<li>Optik</li> 
				<li> Frontiers of Information Technology Electronic Engineering</li>
				<li>PeerJ Computer Science</li>
				<li>吉林大学学报(工学版)</li>
				<li>Image and Vision Computing</li>
				<li>Mathematics </li>
				<li> 中国图象图形学报</li>
				<li>山东科技大学学报</li>
				<li> IET Computer Vision </li>
				<li>Information Sciences</li>
				<li>Knowledge-based Systems</li>
				<li>青岛大学学报（工程技术版）</li>
				<li>IEEE Open Journal of Computer Society </li>
				<li>Multimedia Systems</li>
				<li>Tsinghua Science and Technology</li>
				<li>Journal of Visual Communication and Image Representation</li>
			</ul>
		</li>
		<!--		<li>Assistant reviewer for ECCV2020, CVPR 2020, AAAI 2020, ICCV 2019, VCIP 2018, ICME 2017, ICIP 2017, AVSS 2017, DICTA 2017</li> /-->
		<!--	<li>Volunteer for VCIP 2019, ICML 2017</li>  /-->
    </ul>        
</div>

	<h2>Teaching</h2>
		<ul>
			<li> Cognitive Computing (including experiments), Bachelor Class </li>
			<li> Computional Intelligence, Bachelor Class </li>
			<li> Deep Learning, Postgraduate Class</li>

			<li> Machine Learning, Postgraduate Class</li>
			 <li> Machine Learning Theory, PhD Class, 2020</li>
			<li> Machine Learning, On-job Postgraduate Class</li>
		</ul>

	<h2> Project</h2>
		<ul>
			<li> Elite Talent Program of SDUST, PI, 2020.11-2025.11</li>
			<li> Natural Science Foundation of Shandong Province for Younth, PI, 2022.01-2024.12</li>
			<li> National Natural Science Foundation of China Younth Project, PI, 2023.01-2025.12 </li>
		</ul>
<!--
<div class="text_container">		
    <h2>Honors</h2>
        <ul>
			<li> UTS Industry Scholarship, 2016--2020</li>
			<li> UTS International Research Scholarship, 2016--2020</li>
			<li> Excellent Thesis Award of Shandong Unversity, 2017</li>
			<li> Outstanding Graduate of Shandong Province, 2016 </li>
			<li> National Scholarship, 2015 & 2014 </li>
        </ul>    
</div>
 /-->		
    <h2>Publications</h2>
        <ul>
		<li> Xinyu Zhang, <font color= "#0000FF">Peng Zhang*</font>, Caifeng Shan*.
			Corruption-invariant Person Re-identification Via Coarse-to-Fine Feature Alignment.
			<i>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</i>, 2024.
			[<a href="https://ieeexplore.ieee.org/document/10703072">PDF</a>]
		</li>
		<li>Yongtang Bao, Hao Zheng, <font color= "#0000FF">Peng Zhang*</font>, Caifeng Shan.
		Implicit Feature Augmentation for Cloth-changing Person Re-identification.
		<i>The 18th Chinese Conference on Biometric Recognition (CCBR 2024)</i>, 2024.
		</li>
		<li> Yongtang Bao, Hao Zheng, Xiaolin Zhang, Kun Zhan, <font color= "#0000FF">Peng Zhang*</font>.
			Identity Semantic Correspondence for Cloth-Changing Person Re-Identification.
			<i>2024 International Joint Conference on Neural Networks (IJCNN 2024)</i>, 2024.
			[<a href="https://ieeexplore.ieee.org/document/10651092">PDF</a>]
		</li>
		<li> Yutong He, Yefeng Qin, Lei Chen, <font color= "#0000FF">Peng Zhang</font>, Xianye Ben.
		     Efficient Abnormal Behavior Detection with Adaptive Weight Distribution.
			<i>Neurocomputing</i>, 2024. 
			[<a href="https://www.sciencedirect.com/science/article/pii/S0925231224009585">PDF </a>]
		</li>
		<li> Yongtang Bao, Chenxi Wu, <font color= "#0000FF">Peng Zhang*</font>, Caifeng Shan.
			Adaptive optical flow estimation driven micro-expression recognition.
			<i>Journal of Image and Graphic (JIG) </i>, 2024.
			[<a href="http://txtx.cjig.cn/jig/ch/reader/view_abstract.aspx?flag=2&file_no=202308020000001&journal_id=jig">PDF </a>]
		</li>
		<li> Yongtang Bao, Chenxi Wu, <b><font color= "#0000FF">Peng Zhang*</font></b>, Caifeng Shan, Yue Qi, Xianye Ben. 
		     Boosting Micro-Expression Recognition Via Self-Expression Reconstruction and Memory Contrastive Learning.
		     <i>IEEE Transactions on Affective Computing (IEEE TAFFC)</i>, 2024. 
		     [<a href="https://ieeexplore.ieee.org/abstract/document/10521757">PDF</a>]
		</li>
			
		<li> Zhenyu Liu, Da Li, Xinyu Zhang, Zhang Zhang, <font color= "#0000FF">Peng Zhang</font>, Caifeng Shan, Jungong Han.			
                     Pedestrian Attribute Recognition via Spatio-temporal Relationship Learning for Visual Surveillance.
			<i> ACM Transactions on Multimedia Computing, Communications and Applications (ACM TOMM)</i>, 2024.
			[<a href="https://dl.acm.org/doi/abs/10.1145/3632624">PDF</a>]
		</li>
		<li>Yixuan Ma, Xiaolin Zhang, <font color= "#0000FF">Peng Zhang</font>, Kun Zhan.
		Entropy neural estimation for graph contrastive learning.
			<i>ACM International Conference on Multimedia (ACM MM)</i>, 2023. 
			[<a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612047"> PDF</a>]
		</li>
		<li>Zhibo Tian, Xiaolin Zhang, <font color= "#0000FF">Peng Zhang</font>, Kun Zhan.
		    Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network.
			<i>ACM International Conference on Multimedia (ACM MM)</i>, 2023. 
			[<a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611816"> PDF</a>]
		</li>
		<li>Mingjie Li, Yuan-Gen Wang, <font color= "#0000FF">Peng Zhang</font>, Hanpin Wang, Lisheng Fan, Enxia Li, Wei Wang.
		Deep learning for approximate nearest neighbour search: A survey and future directions.
			<i>IEEE Transactions on Knowledge and Data Engineering (IEEE TKDE)</i>, 2023.
			[<a href="https://ieeexplore.ieee.org/abstract/document/9942356"> PDF</a>]
		</li>
		<li> Lingxiang Yao, Worapan Kusakunniran, <font color= "#0000FF">Peng Zhang</font>, Qiang Wu, Jian Zhang. 
		     Improving Disentangled Representation Learning for Gait Recognition using Group Supervision.
		    <i>IEEE Transactions on Multimedia (IEEE TMM)</i>, 2023. [<a href="https://ieeexplore.ieee.org/document/9767609">PDF</a>]
		</li>
                <li>Yongtang Bao, Pengfei Zhou, <font color= "#0000FF">Peng Zhang</font>, Yue Qi.
		Learning Unoccluded Face Texture Completion from Single Image in the Wild.
			<i>Neural Processing Letters (NPL)</i>, 2023.
			[<a href="https://link.springer.com/article/10.1007/s11063-022-10861-2"> PDF</a>]
		</li>
		<li> <font color= "#0000FF">Peng Zhang</font>, Xiaolin Zhang, Yongtang Bao, Xianye Ben, Caifeng Shan. 
			Cloth-changing Person Re-identification： A Summary
			<i>Journal of Image and Graphic (JIG) </i>, 2023.
			[<a href="https://dx.doi.org/10.11834/jig.220702"> PDF</a>]
		</li>
		<li> Zhenyu Liu, Zhang Zhang, Da Li, <font color= "#0000FF">Peng Zhang</font>, Caifeng Shan.
			Dual-branch self-attention network for pedestrian attribute recognition.
			<i>Pattern Recognition Letters (PRL)</i>, 2022.
			[<a href="https://www.sciencedirect.com/science/article/pii/S0167865522002963">PDF</a>]
		</li>
		<li>
		     Yan Huang, Qiang Wu, Jingsong Xu, Yi Zhong, <font color= "#0000FF">Peng Zhang</font>, Zhaoxiang Zhang.
		     Alleviating Modality Bias Training for Infrared-Visible Person Re-identification.
		     <i>IEEE Transactions on Multimedia (IEEE TMM)</i>,  2022. [<a href="https://doi.org/10.1109/TMM.2021.3067760">PDF</a>]
		</li>
		<li>
		    <font color= "#0000FF">Peng Zhang</font>, Jingsong Xu, Qiang Wu, Yan Huang, Xianye Ben.
		     Learning Spatial-temporal Representations over Walking Tracklet for Long-term Person Re-Identification in The Wild.</b>
		     <i>IEEE Transactions on Multimedia (IEEE TMM)</i>, 2021. [<a href="https://ieeexplore.ieee.org/abstract/document/9214519">PDF</a>]
		</li>
		<li>
		  <font color= "#0000FF">Peng Zhang</font>, Qiang Wu, Xunxiang Yao, Jingsong Xu.
		  Beyond Madality Alignment: Learning Part-level representation for Visible-infrared Person Re-identification.
		  <i>Image and Vision Computing (IVC)</i>, 2021. [<a href="https://doi.org/10.1016/j.imavis.2021.104118">PDF</a>]
		</li>
		<li>  Xunxiang Yao, Qiang Wu,  <font color= "#0000FF">Peng Zhang</font>, Fangxun Bao.
		      Weighted Adaptive Image Super-Resolution Scheme based on Local Fractal Feature and Image Roughness.
			<i>IEEE Transactions on Multimedia (IEEE TMM)</i>, 2021. [<a href="https://ieeexplore.ieee.org/abstract/document/9099392">PDF</a>]
		</li>
		<li> Yan Huang, Jingsong Xu, Qiang Wu, Yi Zhong, <font color= "#0000FF">Peng Zhang</font>, Zhaoxiang Zhang.
		      Beyond Scalar Neuron: Adopting Vector-Neuron Capsules for Long-Term Person Re-Identification.
		      <i>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</i>, 2020. 
		      [<a href="https://ieeexplore.ieee.org/document/8873614">PDF</a>]
		</li>
		<li> <font color= "#0000FF">Peng Zhang</font>, Jingsong Xu, Qiang Wu, Yan Huang, Jian Zhang.
		     Top-Push Constrained Modality-Adaptive Dictionary Learning for Cross-Modality Person Re-Identification.
		     <i>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</i>, 2020.
		     [<a href="https://ieeexplore.ieee.org/document/8825984">PDF</a>]
		</li>
		
		<li><font color= "#0000FF">Peng Zhang</font>, Qiang Wu, Jingsong Xu.
			VT-GAN: View Transformation GAN for Gait Recognition Across View.
			<i>The International Joint Conference on Neural Network(IJCNN)</i>, 2019.
			[<a href="">Oral</a>][<a href="https://ieeexplore.ieee.org/document/8852258">PDF</a>]
		</li>
		<li><font color= "#0000FF">Peng Zhang</font>, Qiang Wu, Jingsong Xu.
			VN-GAN: Identity-preserved Variation Normalizing GAN for Gait Recognition.
			<i>The International Joint Conference on Neural Network(IJCNN)</i>, 2019.
			[<a href="">Poster</a>][<a href="https://ieeexplore.ieee.org/document/8852401">PDF</a>]
		</li>
		<li> Xianye Ben,<font color= "#0000FF">Peng Zhang</font>, Zhihui Lai, Rui Yan, Xinliang Zhai, Weixiao Meng.
		     A General Tensor Representation Framework for Cross-view Gait Recognition.
			<i>Pattern Recognition (PR)</i>, 2019.
			[<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320319300251">PDF</a>]
		</li>
		<li> Xianye Ben, Chen Gong, <font color= "#0000FF">Peng Zhang</font>, Xitong Jia, Qiang Wu, Weixiao Meng.
			Coupled Patch Alignment for Matching Cross-View Gaits.
			<i>IEEE Transactions on Image Processing (IEEE TIP)</i>, 2019.
			[<a href="https://ieeexplore.ieee.org/abstract/document/8624554">PDF</a>]
		</li>
		<li> Xianye Ben, Chen Gong, <font color= "#0000FF">Peng Zhang</font>, Rui Yan, Qiang Wu, Weixiao Meng.
			Coupled Bilinear Discriminant Projection for Cross-view Gait Recognition.
			<i>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</i>, 2019.
			[<a href="https://ieeexplore.ieee.org/abstract/document/8616800">PDF</a>]
		</li>
		<li> Xunxiang Yao, Qiang Wu, <font color= "#0000FF">Peng Zhang</font>, Fangxun Bao.
			Adaptive Rational Fractal Interpolation Function for Image Super-resolution via Local Fractal Analysis.
			<i>Image and Vision Computing</i>, 2019.
			[<a href="https://www.sciencedirect.com/science/article/pii/S0262885619300101">PDF</a>]
		</li>
	
		<li> <font color= "#0000FF">Peng Zhang</font>, Qiang Wu, Jingsong Xu, Jian Zhang.
			Long-term Person Re-identification Using True Motion from Videos.
			<i>IEEE Winter Conference on Applications of Computer Vision (WACV)</i>, 2018.
			[<a href="https://youtu.be/ZGZ-pMfdOz0">Oral</a>][<a href="https://ieeexplore.ieee.org/abstract/document/8354164">PDF</a>]
		</li>
		<li> Xianye Ben, <font color= "#0000FF">Peng Zhang</font>, Weixiao Meng, Rui Yan, Mingqiang Yang, Wenhe Liu, Hui Zhang.
			On The Distance Metric Learning Between Cross-domain Gaits.
			<i>Neurocomputing</i>, 2016.
			[<a href="https://www.sciencedirect.com/science/article/pii/S0925231216304672">PDF</a>]
		</li>
		<li> Xianye Ben, <font color= "#0000FF">Peng Zhang</font>, Rui Yan, Mingqiang Yang, Guodong Ge.
			Gait Recognition  And  Micro-expression Recognition Based on Maximum Margin Projection with Tensor Representation.
			<i>Neural Computing and Applications (NCA), 2016.
			[<a href="https://link.springer.com/article/10.1007/s00521-015-2031-8">PDF</a>]
		</li>
		<li> <font color= "#0000FF">Peng Zhang</font>, Xianye Ben, Rui Yan, Chen Wu, Chang Guo.
			Micro-expression Recognition System. <i>Optik</i>, 2016.
			[<a href="https://www.sciencedirect.com/science/article/pii/S0030402615016009">PDF</a>]
		</li>
		<li><font color= "#0000FF">Peng Zhang</font>, Xianye Ben, Wei Jiang, Rui Yan, Yiming Zhang.
			Coupled Marginal Discriminant Mappings for Low-resolution Face Recognition.
			<i>Optik</i>,  2015. [<a href="https://www.sciencedirect.com/science/article/pii/S0030402615008992">PDF</a>]
		</li>
		

        </ul>    

		
<div class="papers-container papers-selected"> 
	<!--<h5 class="paperlo">All Publications<button type="button" class="ml-3 btn btn-light"> Show selected</button></h5>
	<h5 class="paperhi paperhi-only">Selected Publications<button type="button" class="ml-3 btn btn-light"> Show all</button></h5>

	<!--   
	<div class="publication media paperhi"> 
           <img src="imgs/gan.gif" class="papericon">
           <div class="media-body">
			<strong>Temporal Relational Reasoning in Videos.</strong><br>
           <u>Bolei Zhou</u>, Alex Andonian, Aude Oliva, and Antonio Torralba<br>European Conference on Computer Vision (ECCV), 2018.<br>[<a href="publication/eccv18-TRN.pdf">PDF</a>][<a href="https://arxiv.org/pdf/1711.08496.pdf">arXiv</a>][<a href="http://relation.csail.mit.edu">Webpage</a>][<a href="https://www.youtube.com/watch?v=D42erLb42_k">Demo Video</a>][<a
               href="https://github.com/metalbubble/TRN-pytorch">Code</a>][<a href="http://news.mit.edu/2018/machine-learning-video-activity-recognition-0914">MIT News</a>]
	</div></div>
	-->

	<!--
	<h2>Teaching</h2>
		<ul>
			<li> Machine Learning Theory, PhD Class 2020</li>
			<li> Machine Learning, On-job Postgraduate Class 2021</li>
		</ul>

	<h2> Project</h2>
		<ul>
			<li> Elite Talent Program of SDUST, 2020.11-2025.11</li>
			<li>Natural Science Foundation of Shandong Province for Younth, 2022.01-2024.12</li>
		</ul>
	-->
		<!--
<div class="text_container">
    <h2>Talks</h2>
	<div>
    <ul>
        <li><a href="ppt/presentation_ICML_workshop.pdf">Interpreting Deep Visual Representations</a> at <a href="http://icmlviz.github.io/">Workshop on Visualization for Deep Learning</a>, ICML'17, Sydney.</li>
        <li><a href="ppt/presentation_CVPR17_oraltalk.pdf">Network Dissection: Quantifying the Interpretability of Deep Visual Representations</a>, CVPR'17, Hawaii.</li>
        <li><a href="http://deeplearning.csail.mit.edu/">Tutorial on the Deep Learning for Objects and Scenes</a>, CVPR'17, Hawaii.</li>
        <li><a href="ppt/understandCNN_tufts.pdf">Understand and Leverage the Internal Representations of CNNs</a> at Tufts, Cornell Tech, Harvard. </li>
        <li><a href="publication/scene_challenges2016.pdf">Challenges in Deep Sceen Understanding</a> at ECCV'16 ILSVRC and COCO joint workshop, Oct. 2016, Amsterdam.</li> 
        <li><a href="http://places.csail.mit.edu/slide_iclr2015.pdf">Object Detectors Emerge in Deep Scene CNNs</a> at ICLR'15, May 2015, San Diego.</li>
        <li><a href="">Learning Deep Features for Scene Recognition</a> at NIPS'14, Dec. 2014, Montreal.</li>
        <li><a href="http://mmlab.ie.cuhk.edu.hk/projects/collectiveness/presentation_cvpr2013.pdf">Measuring Crowd Collectiveness</a> at CVPR'13, June 2013, Portland.</li>
        <li><a href="http://mmlab.ie.cuhk.edu.hk/projects/dynamicagent/presentation_ppt.pdf">Understanding Crowd Behaviors</a> at CVPR'12, June 2012, Rhode Island.</li>
    </ul>
	</div>
</div>
-->
<!--
<div class="text_container">
    <h2>Media coverage</h2>
	<div>
    <ul>
        <li><a href="https://venturebeat.com/2018/09/14/mit-csail-designs-ai-that-can-track-objects-over-time/">VentureBeat</a>: MIT CSAIL designs AI that can track objects over time.</li>
        <li><a href="http://news.mit.edu/2018/machine-learning-video-activity-recognition-0914">MIT News</a>: Helping computers fill in the gaps between video frames.</li>
        <li><a href="https://qz.com/1022156/mit-researchers-can-now-track-artificial-intelligences-decisions-back-to-single-neurons/">Quartz</a>: Track AI decisions back to single neurons.</li>
        <li><a href="http://news.mit.edu/2017/inner-workings-neural-networks-visual-data-0630">MIT News</a>: Peering into neural networks.</li>
        <li><a href="https://techcrunch.com/2017/06/30/mit-csail-research-offers-a-fully-automated-way-to-peer-inside-neural-nets/">TechCrunch</a>: A fully automated way to peer inside neural networks.</li>
        <li><a href="https://www.csail.mit.edu/csail_computer_vision_team_leads_scene_parsing_challenge%20">MIT CSAIL News</a>: Scene parsing and scene classification challenges.</li>
        <li><a href="http://techcrunch.com/2015/05/08/ai-project-designed-to-recognize-scenes-surprises-by-identifying-objects-too/" target="_blank">TechCrunch</a> and <a href="http://newsoffice.mit.edu/2015/visual-scenes-object-recognition-0508" target="_blank">MIT News</a>: Object detectors emerge in CNNs.</li>
    </ul>
	</div>
</div>
-->

<!--
<div class="text_container">		
    <h2>Honors</h2>
	<div>
        <ul>
			<li> UTS Industry Scholarship, 2016-2020</li>
			<li> UTS International Research Scholarship, 2016-2020</li>
			<li> Excellent Thesis Award of Shandong Unversity, 2017</li>
			<li> Outstanding Graduate of Shandong Province, 2016 </li>
			<li> National Scholarship, 2015 & 2014 </li>
        </ul>    
	</div>
</div>

<div class="text_container"> 
    <h2>Datasets & Benchmarks</h2>
	<div>	
    <ul>
		<li>TODO...</li>
    </ul>
	</div>
</div>

-->
<!--
    <h2>Collaborators</h2>
    <ul>
        <li>I am fortunate to work with these great people: <a href="http://cvcl.mit.edu/Aude.htm">Aude Oliva</a>(MIT), <a href="http://vision.princeton.edu/people/xj/">Jianxiong Xiao</a>(Princeton), <a href="http://www.cvc.uab.es/~agata/">Agata Lapedriza</a>(UOC), <a href="http://dusp.mit.edu/faculty/jinhua-zhao">Jinhua Zhao</a>(MIT), <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>(CUHK), <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Xiaoou Tang</a>(CUHK), <a href="http://ins.sjtu.edu.cn/faculty/zhanghepeng">Hepeng Zhang</a>(SJTU), <a href="http://bcmi.sjtu.edu.cn/~zhangliqing/">Liqing Zhang</a>(SJTU), <a href="http://www.houxiaodi.com/">Xiaodi Hou</a>(Caltech), <a href="http://liuliu.us/">Liu Liu</a>(MIT), <a href="http://people.csail.mit.edu/khosla/">Aditya Khosla (MIT)</a>, Robinson Piramuthu(eBay Research Labs), Vignesh Jagadeesh(eBay Research Labs), Yuandong Tian(FB), Rob Fergus(NYU&FB), Arthur Szlam(FB), Sainbayar
        Sukhbaatar(NYU), Zi Wang (MIT), Stefanie Jegelka (MIT), Hang Zhao (MIT), Xavier Puig (MIT), Sanja Fidler (UToronto), Larry Zitnick(FB).</li>
    </ul>
<div class="text_container">
    <h2>Personal interests</h2>
    <ul>
    <li>blogs:<a href="http://urbancomputation.wordpress.com/" target="_parent">Urban Computation</a>,<a href="https://crowdbehaviordotorg.wordpress.com/" target="_parent">Crowd Behavior &amp; Psychology</a></li>
    <li><a href="book.html">books</a>, <a href="image/beacon_hill.jpg">rock climbing (5.11C,V6)</a>, <a href="bolei_juggle.mp4">juggling</a> (recently), <a href="image/bass.jpg">bass player</a> (former <a href="image/i3.jpg">lead guitarist</a>)</a> </li>
    </ul>
</div>

   --> 

</div>
</div>
</body></html>

